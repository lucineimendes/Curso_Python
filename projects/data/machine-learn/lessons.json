[
    {
        "id": "ml-intro-conceitos",
        "course_id": "machine-learning",
        "title": "Módulo 1: Fundamentos de Machine Learning",
        "order": 1,
        "description": "Introdução aos conceitos fundamentais de Machine Learning, os tipos de aprendizado e o fluxo de trabalho de um projeto de ML.",
        "learning_objectives": [
            "Definir Machine Learning (ML).",
            "Diferenciar Aprendizado Supervisionado, Não Supervisionado e por Reforço.",
            "Compreender as etapas de um projeto de ML: Coleta, Limpeza, Exploração, Modelagem e Avaliação."
        ],
        "key_concepts": [
            "Machine Learning",
            "Aprendizado Supervisionado",
            "Aprendizado Não Supervisionado",
            "Aprendizado por Reforço",
            "Fluxo de Trabalho de ML"
        ],
        "content": "<h2>O que é Machine Learning?</h2><p>Machine Learning (Aprendizado de Máquina) é um subcampo da Inteligência Artificial que dá aos computadores a habilidade de aprender sem serem explicitamente programados. O foco é desenvolver algoritmos que podem acessar dados e usá-los para aprender por si mesmos.</p><h3>Tipos de Aprendizado</h3><ul><li><strong>Supervisionado:</strong> O algoritmo aprende a partir de um conjunto de dados de treinamento rotulado, onde cada exemplo tem uma 'resposta correta' (target). O objetivo é aprender uma função que mapeia as entradas (features) para as saídas (target).</li><li><strong>Não Supervisionado:</strong> O algoritmo trabalha com dados não rotulados para encontrar padrões ou estruturas ocultas.</li><li><strong>Por Reforço:</strong> Um agente aprende a tomar decisões em um ambiente para maximizar uma recompensa cumulativa.</li></ul><h3>O Fluxo de Trabalho de um Projeto de ML</h3><ol><li><strong>Coleta de Dados:</strong> Obtenção dos dados brutos.</li><li><strong>Limpeza e Pré-processamento:</strong> Tratar valores ausentes, corrigir erros e formatar os dados.</li><li><strong>Análise Exploratória de Dados (EDA):</strong> Visualizar e entender os dados.</li><li><strong>Engenharia de Features:</strong> Selecionar e criar as variáveis de entrada para o modelo.</li><li><strong>Modelagem:</strong> Escolher, treinar e ajustar um modelo de ML.</li><li><strong>Avaliação:</strong> Medir o desempenho do modelo.</li><li><strong>Implantação (Deployment):</strong> Colocar o modelo em produção.</li></ol>",
        "examples": [],
        "summary": "Machine Learning permite que sistemas aprendam com dados. Os principais tipos são Supervisionado (com dados rotulados), Não Supervisionado (sem rótulos) e por Reforço. O fluxo de um projeto envolve coletar dados, treinar e avaliar um modelo.",
        "estimated_time_minutes": 30
    },
    {
        "id": "ml-regressao-linear",
        "course_id": "machine-learning",
        "title": "Módulo 2: Regressão Linear",
        "order": 2,
        "description": "Aprenda seu primeiro modelo de Machine Learning: Regressão Linear. Entenda a teoria e como implementá-lo com Scikit-learn para prever valores contínuos.",
        "learning_objectives": [
            "Entender o conceito de Regressão Linear Simples e Múltipla.",
            "Dividir dados em conjuntos de treino e teste usando `train_test_split`.",
            "Treinar um modelo de Regressão Linear com Scikit-learn.",
            "Avaliar o modelo usando métricas como MSE e R²."
        ],
        "key_concepts": [
            "Regressão Linear",
            "Scikit-learn",
            "Conjunto de Treino e Teste",
            "`train_test_split`",
            "`LinearRegression`",
            "Erro Quadrático Médio (MSE)",
            "R² (Coeficiente de Determinação)"
        ],
        "content": "<h2>Regressão Linear: Prevendo Valores Contínuos</h2><p>A Regressão Linear é um dos algoritmos mais simples e fundamentais de ML. O objetivo é encontrar a melhor linha reta (ou hiperplano, em múltiplas dimensões) que se ajusta aos dados, descrevendo a relação entre as features (variáveis independentes) e o target (variável dependente).</p><h3>Implementação com Scikit-learn</h3><p>Scikit-learn é a principal biblioteca de ML em Python. O fluxo de trabalho é muito consistente entre diferentes modelos.</p><pre><code class='language-python'>from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# 1. Dividir dados em treino e teste\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 2. Criar e treinar o modelo\nmodelo = LinearRegression()\nmodelo.fit(X_train, y_train)\n\n# 3. Fazer previsões com os dados de teste\ny_pred = modelo.predict(X_test)\n\n# 4. Avaliar o modelo\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)</code></pre>",
        "examples": [],
        "summary": "Regressão Linear é um modelo supervisionado para prever valores contínuos. Com Scikit-learn, o processo envolve dividir os dados em treino/teste, instanciar o modelo `LinearRegression`, treiná-lo com `.fit()`, prever com `.predict()` e avaliar com métricas como MSE e R².",
        "estimated_time_minutes": 45
    },
    {
        "id": "ml-regressao-polinomial",
        "course_id": "machine-learning",
        "title": "Módulo 2: Regressão Polinomial",
        "order": 3,
        "description": "Explore a Regressão Polinomial para modelar relações não-lineares entre variáveis.",
        "learning_objectives": [
            "Entender quando a Regressão Polinomial é apropriada.",
            "Usar `PolynomialFeatures` do Scikit-learn para criar features polinomiais.",
            "Treinar um modelo de Regressão Linear com as novas features."
        ],
        "key_concepts": [
            "Regressão Polinomial",
            "Relações Não-Lineares",
            "Overfitting",
            "`PolynomialFeatures`"
        ],
        "content": "<h2>Regressão Polinomial</h2><p>Quando a relação entre as variáveis de entrada e saída não é linear, uma linha reta não consegue capturar bem o padrão dos dados. A Regressão Polinomial estende a regressão linear adicionando termos polinomiais (x², x³, etc.) às features. Isso permite que o modelo se ajuste a curvas.</p><pre><code class='language-python'>from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\n# 1. Criar features polinomiais\npoly_features = PolynomialFeatures(degree=2) # Grau do polinômio\nX_poly = poly_features.fit_transform(X)\n\n# 2. Treinar um modelo de Regressão Linear com as novas features\nmodelo_poly = LinearRegression()\nmodelo_poly.fit(X_poly, y)</code></pre><p><strong>Cuidado:</strong> Graus muito altos podem levar a overfitting, onde o modelo se ajusta perfeitamente aos dados de treino, mas generaliza mal para novos dados.</p>",
        "examples": [],
        "summary": "Regressão Polinomial é usada para modelar relações curvilíneas, transformando as features originais em termos polinomiais e aplicando uma regressão linear sobre elas.",
        "estimated_time_minutes": 45
    },
    {
        "id": "ml-regressao-logistica",
        "course_id": "machine-learning",
        "title": "Módulo 2: Regressão Logística",
        "order": 4,
        "description": "Entenda como a Regressão Logística, apesar do nome, é um poderoso algoritmo de classificação.",
        "learning_objectives": [
            "Compreender que a Regressão Logística é um modelo de classificação.",
            "Entender o papel da função sigmoide.",
            "Treinar e avaliar um modelo de Regressão Logística com Scikit-learn."
        ],
        "key_concepts": [
            "Regressão Logística",
            "Classificação Binária",
            "Função Sigmoide",
            "`LogisticRegression`"
        ],
        "content": "<h2>Regressão Logística para Classificação</h2><p>A Regressão Logística é um algoritmo de aprendizado supervisionado usado para problemas de <strong>classificação</strong>. O nome 'regressão' vem do fato de que o modelo prevê a probabilidade de uma instância pertencer a uma classe. Essa probabilidade é então convertida em uma classificação (ex: se a probabilidade for > 0.5, classe 1; senão, classe 0).</p><p>O modelo usa a <strong>função sigmoide</strong> para mapear qualquer valor real para um valor entre 0 e 1, que é interpretado como a probabilidade.</p><pre><code class='language-python'>from sklearn.linear_model import LogisticRegression\n\n# X e y são seus dados de features e classes (0 ou 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nmodelo_logistico = LogisticRegression()\nmodelo_logistico.fit(X_train, y_train)\n\ny_pred = modelo_logistico.predict(X_test)</code></pre>",
        "examples": [],
        "summary": "Regressão Logística é um modelo de classificação que prevê a probabilidade de um resultado categórico. Ele usa a função sigmoide e é fundamental para problemas de classificação binária.",
        "estimated_time_minutes": 40
    },
    {
        "id": "ml-arvores-decisao",
        "course_id": "machine-learning",
        "title": "Módulo 3: Árvores de Decisão",
        "order": 5,
        "description": "Aprenda sobre Árvores de Decisão, um modelo intuitivo e poderoso para classificação e regressão.",
        "learning_objectives": [
            "Entender a estrutura de uma Árvore de Decisão (nós, ramos, folhas).",
            "Compreender como uma árvore toma decisões.",
            "Treinar um classificador de Árvore de Decisão com Scikit-learn."
        ],
        "key_concepts": [
            "Árvore de Decisão",
            "Nó Raiz, Nó Interno, Nó Folha",
            "Critérios de Divisão (Gini, Entropia)",
            "`DecisionTreeClassifier`"
        ],
        "content": "<h2>Árvores de Decisão</h2><p>Uma Árvore de Decisão é um modelo que se assemelha a um fluxograma. Cada nó interno representa um 'teste' em um atributo (ex: 'a cor é azul?'), cada ramo representa o resultado do teste, e cada nó folha representa um rótulo de classe. O caminho da raiz até uma folha representa as regras de classificação.</p><p>São modelos fáceis de interpretar e visualizar, mas uma única árvore pode ser propensa a overfitting.</p><pre><code class='language-python'>from sklearn.tree import DecisionTreeClassifier\n\nmodelo_arvore = DecisionTreeClassifier()\nmodelo_arvore.fit(X_train, y_train)\n\ny_pred = modelo_arvore.predict(X_test)</code></pre>",
        "examples": [],
        "summary": "Árvores de Decisão são modelos de ML que aprendem regras de decisão simples a partir dos dados. São intuitivas e fáceis de visualizar, mas podem sofrer de overfitting.",
        "estimated_time_minutes": 45
    },
    {
        "id": "ml-random-forest",
        "course_id": "machine-learning",
        "title": "Módulo 3: Random Forest",
        "order": 6,
        "description": "Descubra o poder dos métodos de ensemble com o Random Forest, um dos algoritmos mais populares e eficazes.",
        "learning_objectives": [
            "Entender o conceito de 'ensemble learning' e 'bagging'.",
            "Compreender como o Random Forest combina múltiplas árvores de decisão.",
            "Treinar um classificador Random Forest com Scikit-learn."
        ],
        "key_concepts": [
            "Random Forest (Floresta Aleatória)",
            "Ensemble Learning",
            "Bagging (Bootstrap Aggregating)",
            "`RandomForestClassifier`"
        ],
        "content": "<h2>Random Forest</h2><p>Random Forest é um método de <strong>ensemble learning</strong>. Em vez de depender de um único modelo, ele constrói múltiplas árvores de decisão durante o treinamento e produz a classe que é o modo das classes (classificação) ou a previsão média (regressão) das árvores individuais.</p><p>Ele usa uma técnica chamada <strong>bagging</strong> para treinar cada árvore em uma amostra aleatória dos dados, o que ajuda a reduzir a variância e o overfitting, tornando-o um modelo muito mais robusto do que uma única árvore de decisão.</p><pre><code class='language-python'>from sklearn.ensemble import RandomForestClassifier\n\nmodelo_rf = RandomForestClassifier(n_estimators=100) # n_estimators = número de árvores\nmodelo_rf.fit(X_train, y_train)\n\ny_pred = modelo_rf.predict(X_test)</code></pre>",
        "examples": [],
        "summary": "Random Forest é um poderoso algoritmo de ensemble que constrói múltiplas árvores de decisão para melhorar a precisão e controlar o overfitting.",
        "estimated_time_minutes": 50
    },
    {
        "id": "ml-svm",
        "course_id": "machine-learning",
        "title": "Módulo 3: SVM (Máquinas de Vetores de Suporte)",
        "order": 7,
        "description": "Aprenda sobre Máquinas de Vetores de Suporte (SVM), um classificador eficaz que encontra o melhor hiperplano para separar classes.",
        "learning_objectives": [
            "Entender a intuição por trás do SVM e o conceito de margem máxima.",
            "Compreender o 'truque do kernel' para lidar com dados não linearmente separáveis.",
            "Treinar um classificador SVM com Scikit-learn."
        ],
        "key_concepts": [
            "SVM (Support Vector Machine)",
            "Hiperplano",
            "Margem",
            "Vetores de Suporte",
            "Kernel Trick",
            "`SVC` (Support Vector Classifier)"
        ],
        "content": "<h2>Máquinas de Vetores de Suporte (SVM)</h2><p>O objetivo de um SVM é encontrar um <strong>hiperplano</strong> em um espaço de N dimensões (onde N é o número de features) que classifica distintamente os pontos de dados. O melhor hiperplano é aquele que tem a maior distância (margem) para os pontos de dados mais próximos de qualquer classe (chamados de vetores de suporte).</p><p>Para dados que não são linearmente separáveis, o SVM usa o <strong>'truque do kernel'</strong> para mapear os dados para um espaço de dimensão superior onde uma separação linear se torna possível.</p><pre><code class='language-python'>from sklearn.svm import SVC\n\nmodelo_svm = SVC(kernel='rbf') # 'rbf' é um kernel comum para dados não lineares\nmodelo_svm.fit(X_train, y_train)\n\ny_pred = modelo_svm.predict(X_test)</code></pre>",
        "examples": [],
        "summary": "SVM é um classificador poderoso que funciona encontrando o hiperplano que maximiza a margem entre as classes, sendo eficaz em espaços de alta dimensão.",
        "estimated_time_minutes": 50
    },
    {
        "id": "ml-metricas-classificacao",
        "course_id": "machine-learning",
        "title": "Módulo 4: Métricas de Avaliação de Classificação",
        "order": 8,
        "description": "Aprenda a avaliar o desempenho de seus modelos de classificação com métricas essenciais como Acurácia, Precisão, Recall e F1-Score.",
        "learning_objectives": [
            "Entender a Matriz de Confusão (Verdadeiros Positivos, Falsos Positivos, etc.).",
            "Calcular e interpretar Acurácia, Precisão, Recall e F1-Score.",
            "Entender quando cada métrica é mais importante."
        ],
        "key_concepts": [
            "Matriz de Confusão",
            "Acurácia",
            "Precisão",
            "Recall (Sensibilidade)",
            "F1-Score"
        ],
        "content": "<h2>Avaliando Modelos de Classificação</h2><p>Acurácia nem sempre é a melhor métrica, especialmente com dados desbalanceados. Outras métricas são cruciais:</p><ul><li><strong>Acurácia:</strong> (VP+VN) / Total. Proporção de previsões corretas.</li><li><strong>Precisão:</strong> VP / (VP+FP). Das previsões positivas, quantas estavam corretas? Importante quando o custo de um Falso Positivo é alto.</li><li><strong>Recall (Sensibilidade):</strong> VP / (VP+FN). De todos os positivos reais, quantos o modelo encontrou? Importante quando o custo de um Falso Negativo é alto.</li><li><strong>F1-Score:</strong> Média harmônica de Precisão e Recall. Bom para um balanço entre as duas.</li></ul><pre><code class='language-python'>from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n\n# y_test são os valores reais, y_pred são as previsões do modelo\nprint(f'Acurácia: {accuracy_score(y_test, y_pred)}')\nprint(f'Precisão: {precision_score(y_test, y_pred)}')\nprint(f'Recall: {recall_score(y_test, y_pred)}')\nprint(f'F1-Score: {f1_score(y_test, y_pred)}')\nprint(f'Matriz de Confusão:\\n{confusion_matrix(y_test, y_pred)}')</code></pre>",
        "examples": [],
        "summary": "A avaliação de modelos de classificação vai além da acurácia. Precisão, Recall e F1-Score fornecem uma visão mais completa do desempenho, especialmente em cenários com dados desbalanceados.",
        "estimated_time_minutes": 40
    },
    {
        "id": "ml-validacao-cruzada",
        "course_id": "machine-learning",
        "title": "Módulo 4: Validação Cruzada",
        "order": 9,
        "description": "Aprenda a usar a Validação Cruzada para obter uma estimativa mais robusta do desempenho do seu modelo.",
        "learning_objectives": [
            "Entender a limitação da divisão simples treino/teste (Holdout).",
            "Compreender o processo de K-Fold Cross-Validation.",
            "Aplicar a validação cruzada com Scikit-learn."
        ],
        "key_concepts": [
            "Validação Cruzada (Cross-Validation)",
            "K-Fold",
            "`cross_val_score`"
        ],
        "content": "<h2>Validação Cruzada</h2><p>Uma única divisão treino/teste pode ser sensível à forma como os dados foram divididos. A <strong>Validação Cruzada K-Fold</strong> resolve isso dividindo os dados em 'K' subconjuntos (folds). O modelo é treinado K vezes, cada vez usando um fold diferente como conjunto de teste e os K-1 restantes como treino. A performance final é a média das K performances.</p><pre><code class='language-python'>from sklearn.model_selection import cross_val_score\n\n# modelo é o seu classificador (ex: RandomForestClassifier)\n# X e y são seus dados completos\nscores = cross_val_score(modelo, X, y, cv=5) # cv=5 para 5-fold\n\nprint(f'Scores de cada fold: {scores}')\nprint(f'Acurácia média: {scores.mean()}')</code></pre>",
        "examples": [],
        "summary": "A Validação Cruzada K-Fold fornece uma avaliação de desempenho mais confiável do que uma única divisão treino/teste, treinando e testando o modelo em múltiplas partições dos dados.",
        "estimated_time_minutes": 35
    },
    {
        "id": "ml-otimizacao-hiperparametros",
        "course_id": "machine-learning",
        "title": "Módulo 4: Otimização de Hiperparâmetros",
        "order": 10,
        "description": "Aprenda a encontrar a melhor combinação de hiperparâmetros para seus modelos usando Grid Search e Random Search.",
        "learning_objectives": [
            "Diferenciar parâmetros de modelo e hiperparâmetros.",
            "Implementar Grid Search para uma busca exaustiva.",
            "Implementar Random Search para uma busca mais eficiente."
        ],
        "key_concepts": [
            "Hiperparâmetros",
            "Grid Search",
            "Random Search",
            "`GridSearchCV`",
            "`RandomizedSearchCV`"
        ],
        "content": "<h2>Otimização de Hiperparâmetros</h2><p><strong>Hiperparâmetros</strong> são configurações do algoritmo que não são aprendidas dos dados (ex: o número de árvores em um Random Forest). A escolha correta pode impactar significativamente o desempenho do modelo.</p><ul><li><strong>Grid Search:</strong> Testa exaustivamente todas as combinações de hiperparâmetros que você define.</li><li><strong>Random Search:</strong> Testa um número fixo de combinações aleatórias. É muitas vezes mais eficiente que o Grid Search.</li></ul><pre><code class='language-python'>from sklearn.model_selection import GridSearchCV\n\nparam_grid = {'n_estimators': [10, 50, 100], 'max_depth': [None, 10, 20]}\nmodelo_rf = RandomForestClassifier()\n\ngrid_search = GridSearchCV(estimator=modelo_rf, param_grid=param_grid, cv=3)\ngrid_search.fit(X_train, y_train)\n\nprint(f'Melhores parâmetros: {grid_search.best_params_}')</code></pre>",
        "examples": [],
        "summary": "A otimização de hiperparâmetros, usando técnicas como Grid Search e Random Search, é crucial para extrair o máximo de desempenho de um modelo de Machine Learning.",
        "estimated_time_minutes": 40
    },
    {
        "id": "ml-kmeans-clustering",
        "course_id": "machine-learning",
        "title": "Módulo 5: Agrupamento com K-Means",
        "order": 11,
        "description": "Explore o aprendizado não supervisionado com o K-Means, um algoritmo popular para agrupar dados.",
        "learning_objectives": [
            "Entender o conceito de agrupamento (clustering).",
            "Compreender o funcionamento do algoritmo K-Means.",
            "Aplicar K-Means com Scikit-learn."
        ],
        "key_concepts": [
            "Agrupamento (Clustering)",
            "K-Means",
            "Centroide",
            "`KMeans`"
        ],
        "content": "<h2>Agrupamento com K-Means</h2><p><strong>Agrupamento</strong> é uma tarefa de aprendizado não supervisionado que visa agrupar um conjunto de objetos de tal forma que objetos no mesmo grupo (chamado de cluster) são mais semelhantes entre si do que com aqueles em outros grupos.</p><p>O <strong>K-Means</strong> é um dos algoritmos de agrupamento mais simples. Ele particiona os dados em 'K' clusters, onde cada ponto de dado pertence ao cluster com a média (centroide) mais próxima.</p><pre><code class='language-python'>from sklearn.cluster import KMeans\n\n# X são seus dados não rotulados\nkmeans = KMeans(n_clusters=3, random_state=42) # n_clusters é o 'K'\nkmeans.fit(X)\n\n# Obtém os rótulos dos clusters para cada ponto\nlabels = kmeans.labels_\n# Obtém as coordenadas dos centroides\ncentroides = kmeans.cluster_centers_</code></pre>",
        "examples": [],
        "summary": "K-Means é um algoritmo de clustering que agrupa dados em K clusters com base na proximidade dos pontos aos centroides dos clusters.",
        "estimated_time_minutes": 45
    },
    {
        "id": "ml-pca",
        "course_id": "machine-learning",
        "title": "Módulo 5: Redução de Dimensionalidade com PCA",
        "order": 12,
        "description": "Aprenda a usar a Análise de Componentes Principais (PCA) para reduzir a dimensionalidade dos seus dados.",
        "learning_objectives": [
            "Entender o problema da 'maldição da dimensionalidade'.",
            "Compreender o objetivo do PCA.",
            "Aplicar PCA com Scikit-learn."
        ],
        "key_concepts": [
            "Redução de Dimensionalidade",
            "PCA (Principal Component Analysis)",
            "Componentes Principais",
            "Variância Explicada",
            "`PCA`"
        ],
        "content": "<h2>Redução de Dimensionalidade com PCA</h2><p>Conjuntos de dados com muitas features (alta dimensionalidade) podem ser difíceis de modelar. A <strong>Análise de Componentes Principais (PCA)</strong> é uma técnica de redução de dimensionalidade que transforma os dados em um novo sistema de coordenadas, onde os eixos (componentes principais) são ordenados pela quantidade de variância que eles capturam nos dados. Ao manter apenas os primeiros componentes, podemos reduzir a dimensionalidade preservando a maior parte da informação.</p><pre><code class='language-python'>from sklearn.decomposition import PCA\n\n# X são seus dados de alta dimensão\npca = PCA(n_components=2) # Reduzir para 2 dimensões\nX_reduzido = pca.fit_transform(X)\n\nprint(f'Variância explicada por cada componente: {pca.explained_variance_ratio_}')</code></pre>",
        "examples": [],
        "summary": "PCA é uma técnica de aprendizado não supervisionado para reduzir a dimensionalidade de dados, encontrando os componentes principais que capturam a maior parte da variância.",
        "estimated_time_minutes": 45
    },
    {
        "id": "ml-transformers",
        "course_id": "machine-learning",
        "title": "Módulo 6: A Arquitetura Transformer",
        "order": 13,
        "description": "Entenda a arquitetura Transformer, que revolucionou o Processamento de Linguagem Natural (PLN) e é a base da IA moderna.",
        "learning_objectives": [
            "Compreender as limitações de arquiteturas sequenciais como RNNs.",
            "Entender os componentes do Transformer: Codificador e Decodificador.",
            "Compreender o mecanismo de Atenção (Attention) e Auto-Atenção (Self-Attention).",
            "Reconhecer a vantagem de paralelização do Transformer."
        ],
        "key_concepts": [
            "Transformer",
            "RNN (Recurrent Neural Network)",
            "Atenção (Attention)",
            "Auto-Atenção (Self-Attention)",
            "Codificador (Encoder)",
            "Decodificador (Decoder)",
            "Paralelização"
        ],
        "content": "<h2>A Arquitetura Transformer</h2><p>A arquitetura Transformer, introduzida no artigo 'Attention Is All You Need', superou as limitações de modelos sequenciais como RNNs e LSTMs, que tinham dificuldade em lidar com dependências de longo prazo e não podiam ser facilmente paralelizados. O componente chave do Transformer é o mecanismo de <strong>Auto-Atenção (Self-Attention)</strong>, que permite ao modelo pesar a importância de diferentes palavras em uma sequência ao processar cada palavra, capturando o contexto de forma muito mais eficaz.</p><h3>Componentes Principais</h3><ul><li><strong>Codificador (Encoder):</strong> Processa a sequência de entrada e a transforma em uma representação rica em contexto.</li><li><strong>Decodificador (Decoder):</strong> Usa a representação do codificador e a saída gerada até o momento para produzir o próximo item na sequência de saída.</li></ul>",
        "examples": [],
        "summary": "O Transformer é uma arquitetura de rede neural baseada no mecanismo de atenção, que permite o processamento paralelo de sequências e captura de dependências de longo prazo, sendo a base para modelos como BERT e GPT.",
        "estimated_time_minutes": 50
    },
    {
        "id": "ml-modelos-transformers",
        "course_id": "machine-learning",
        "title": "Módulo 6: Modelos Baseados em Transformer (BERT, GPT)",
        "order": 14,
        "description": "Conheça as famílias de modelos mais importantes baseadas na arquitetura Transformer, como BERT e GPT.",
        "learning_objectives": [
            "Diferenciar arquiteturas de apenas codificador, apenas decodificador e codificador-decodificador.",
            "Entender o propósito e funcionamento do BERT.",
            "Entender o propósito e funcionamento da família GPT.",
            "Conhecer outros modelos importantes como T5 e BART."
        ],
        "key_concepts": [
            "BERT",
            "GPT",
            "T5",
            "BART",
            "Arquitetura Apenas Codificador",
            "Arquitetura Apenas Decodificador",
            "Arquitetura Codificador-Decodificador"
        ],
        "content": "<h2>Modelos Baseados em Transformer</h2><p>A arquitetura Transformer deu origem a três famílias principais de modelos:</p><ul><li><strong>Apenas Codificador (Encoder-only):</strong> Projetados para entender o contexto. Excelentes para tarefas como classificação de texto e extração de entidades. Exemplo principal: <strong>BERT</strong> (Bidirectional Encoder Representations from Transformers).</li><li><strong>Apenas Decodificador (Decoder-only):</strong> Projetados para gerar texto. São autorregressivos, prevendo a próxima palavra com base nas anteriores. Exemplo principal: Família <strong>GPT</strong> (Generative Pre-trained Transformer).</li><li><strong>Codificador-Decodificador (Encoder-Decoder):</strong> Projetados para tarefas de sequência a sequência, como tradução automática ou sumarização. Exemplos: <strong>T5</strong>, <strong>BART</strong>.</li></ul>",
        "examples": [],
        "summary": "Modelos como BERT (baseado em codificador) são ótimos para tarefas de compreensão de linguagem, enquanto modelos como GPT (baseado em decodificador) são excelentes para geração de texto. Modelos codificador-decodificador são usados para tarefas de transformação de sequência.",
        "estimated_time_minutes": 50
    },
    {
        "id": "ml-embeddings",
        "course_id": "machine-learning",
        "title": "Módulo 6: Codificação de Texto (Embeddings)",
        "order": 15,
        "description": "Aprenda como transformar texto em representações numéricas que os modelos de ML podem entender, desde métodos tradicionais até embeddings neurais.",
        "learning_objectives": [
            "Compreender métodos tradicionais como Bag-of-Words (BOW) e TF-IDF.",
            "Entender o conceito de Word Embeddings.",
            "Conhecer métodos neurais como Word2Vec e Sentence-BERT."
        ],
        "key_concepts": [
            "Embeddings",
            "Bag-of-Words (BOW)",
            "TF-IDF",
            "Word2Vec",
            "Sentence-BERT"
        ],
        "content": "<h2>Codificação de Texto (Embeddings)</h2><p>Modelos de Machine Learning não entendem texto, eles entendem números. O processo de converter texto em vetores numéricos é chamado de embedding.</p><ul><li><strong>Bag-of-Words (BOW):</strong> Representa o texto pela contagem de ocorrências de cada palavra, ignorando a ordem.</li><li><strong>TF-IDF:</strong> Uma melhoria do BOW que pesa as palavras não apenas por sua frequência em um documento, mas também por quão raras elas são em toda a coleção de documentos.</li><li><strong>Word2Vec:</strong> Um método neural que aprende representações vetoriais (embeddings) para palavras. A vantagem é que palavras com significados semelhantes têm vetores próximos no espaço vetorial.</li><li><strong>Sentence-BERT:</strong> Um modelo que cria embeddings para frases ou parágrafos inteiros, capturando o significado contextual da sentença completa.</li></ul>",
        "examples": [],
        "summary": "Embeddings convertem texto em vetores numéricos. Métodos tradicionais como BOW e TF-IDF são baseados em contagem, enquanto métodos neurais como Word2Vec e Sentence-BERT capturam o significado semântico do texto.",
        "estimated_time_minutes": 45
    },
    {
        "id": "ml-intro-deep-learning",
        "course_id": "machine-learning",
        "title": "Módulo 7: Introdução a Redes Neurais e Deep Learning",
        "order": 16,
        "description": "Tenha uma visão geral do que são Redes Neurais e Deep Learning, os blocos de construção da IA moderna.",
        "learning_objectives": [
            "Entender a estrutura básica de uma rede neural (neurônios, camadas, pesos).",
            "Diferenciar Machine Learning tradicional de Deep Learning.",
            "Conhecer os principais tipos de redes neurais (DNN, CNN, RNN)."
        ],
        "key_concepts": [
            "Rede Neural Artificial (ANN)",
            "Deep Learning",
            "Neurônio (Unidade)",
            "Camada de Entrada, Camada Oculta, Camada de Saída",
            "Função de Ativação"
        ],
        "content": "<h2>Introdução a Deep Learning</h2><p><strong>Deep Learning</strong> é um subcampo do Machine Learning baseado em <strong>Redes Neurais Artificiais (ANNs)</strong>. A principal característica do Deep Learning é o uso de redes neurais com muitas camadas (daí o 'deep' - profundo). Essas múltiplas camadas permitem que o modelo aprenda hierarquias de features, desde as mais simples (como bordas em uma imagem) até as mais complexas (como objetos completos).</p><p>Enquanto o ML tradicional muitas vezes requer engenharia de features manual, os modelos de Deep Learning podem aprender as features relevantes diretamente dos dados brutos.</p>",
        "examples": [],
        "summary": "Deep Learning utiliza redes neurais com múltiplas camadas para aprender representações complexas dos dados, sendo a força por trás de muitos avanços recentes em IA.",
        "estimated_time_minutes": 40
    },
    {
        "id": "ml-etica-vies",
        "course_id": "machine-learning",
        "title": "Módulo 7: Ética e Vieses em Machine Learning",
        "order": 17,
        "description": "Discuta as importantes considerações éticas e o problema do viés (bias) em modelos de Machine Learning.",
        "learning_objectives": [
            "Entender como o viés pode ser introduzido nos dados e nos modelos.",
            "Reconhecer as implicações sociais de modelos de ML enviesados.",
            "Conhecer estratégias para mitigar o viés."
        ],
        "key_concepts": [
            "Ética em IA",
            "Viés (Bias)",
            "Justiça (Fairness)",
            "Transparência",
            "Explicabilidade (Explainability)"
        ],
        "content": "<h2>Ética e Vieses em ML</h2><p>Modelos de Machine Learning aprendem a partir dos dados que lhes são fornecidos. Se os dados refletem preconceitos e desigualdades existentes na sociedade, o modelo aprenderá e poderá até amplificar esses preconceitos. Isso pode levar a decisões injustas em áreas críticas como contratação, concessão de crédito e justiça criminal.</p><h3>Fontes de Viés</h3><ul><li><strong>Viés nos Dados:</strong> Os dados de treinamento não representam a realidade ou contêm preconceitos históricos.</li><li><strong>Viés no Algoritmo:</strong> O próprio algoritmo pode favorecer certos resultados.</li><li><strong>Viés de Interação:</strong> A forma como os usuários interagem com o modelo pode criar loops de feedback que reforçam o viés.</li></ul><p>É crucial que desenvolvedores de ML estejam cientes dessas questões e trabalhem ativamente para criar sistemas mais justos, transparentes e explicáveis.</p>",
        "examples": [],
        "summary": "A ética em ML é fundamental. É preciso estar ciente do potencial de viés nos dados e nos modelos e tomar medidas para garantir que os sistemas de IA sejam justos e não perpetuem desigualdades.",
        "estimated_time_minutes": 35
    }
,
    {
        "id": "ml-projeto-final",
        "course_id": "machine-learning",
        "title": "Módulo 8: Projeto Final - Análise Preditiva",
        "order": 18,
        "description": "Aplique todo o conhecimento adquirido no curso para desenvolver um projeto completo de Machine Learning, desde a análise de dados até a avaliação de um modelo preditivo.",
        "learning_objectives": [
            "Consolidar o aprendizado do curso através de um projeto prático de ponta a ponta.",
            "Aplicar o fluxo de trabalho completo de um projeto de Machine Learning em um cenário real.",
            "Realizar análise exploratória de dados, pré-processamento e engenharia de features.",
            "Treinar, avaliar e comparar diferentes modelos de Machine Learning.",
            "Documentar e apresentar os resultados de um projeto de ML.",
            "Desenvolver um portfólio com um projeto significativo."
        ],
        "key_concepts": [
            "Projeto Prático",
            "Workflow de ML",
            "Análise Exploratória de Dados (EDA)",
            "Pré-processamento",
            "Treinamento de Modelo",
            "Avaliação de Modelo",
            "Portfólio"
        ],
        "content": "<h2>Projeto Final: Sistema de Previsão de Preços de Imóveis</h2>\n\n<p>Parabéns por chegar ao projeto final! O objetivo é aplicar os conceitos aprendidos para construir e avaliar um modelo de regressão que prevê o preço de imóveis com base em suas características.</p>\n\n<h3>Requisitos Detalhados:</h3>\n<ol>\n  <li><strong>Definição do Problema e Coleta de Dados:</strong>\n    <ul>\n      <li>Utilize um conjunto de dados clássico para este problema, como o \"Boston Housing Dataset\" ou o \"California Housing Prices\" (ambos disponíveis no Scikit-learn ou em plataformas como o Kaggle).</li>\n    </ul>\n  </li>\n  <li><strong>Análise Exploratória de Dados (EDA):</strong>\n    <ul>\n      <li>Carregue os dados em um DataFrame Pandas.</li>\n      <li>Use <code>.info()</code>, <code>.describe()</code>, <code>.isnull().sum()</code> para inspecionar os dados.</li>\n      <li>Crie visualizações com Matplotlib/Seaborn para entender a distribuição das variáveis e as correlações entre elas (ex: histogramas para features, scatter plot entre features e o preço, heatmap de correlação).</li>\n    </ul>\n  </li>\n  <li><strong>Pré-processamento e Engenharia de Features:</strong>\n    <ul>\n      <li>Lide com valores ausentes, se houver.</li>\n      <li>Divida os dados em conjuntos de treino e teste usando <code>train_test_split</code>.</li>\n      <li>Aplique escalonamento de features (ex: <code>StandardScaler</code>) se necessário, para que os modelos performem melhor.</li>\n    </ul>\n  </li>\n  <li><strong>Modelagem:</strong>\n    <ul>\n      <li>Treine pelo menos dois modelos de regressão diferentes (ex: <code>LinearRegression</code>, <code>RandomForestRegressor</code>, <code>SVR</code>).</li>\n    </ul>\n  </li>\n  <li><strong>Avaliação do Modelo:</strong>\n    <ul>\n      <li>Faça previsões no conjunto de teste para cada modelo.</li>\n      <li>Avalie os modelos usando métricas de regressão apropriadas, como Erro Quadrático Médio (MSE), Raiz do Erro Quadrático Médio (RMSE) e Coeficiente de Determinação (R²).</li>\n      <li>Compare o desempenho dos modelos e discuta qual deles foi o melhor para este problema e por quê.</li>\n    </ul>\n  </li>\n  <li><strong>Entrega:</strong>\n    <ul>\n      <li>O projeto deve ser desenvolvido em um Jupyter Notebook ou em um projeto Python estruturado.</li>\n      <li>O código deve ser claro e bem comentado.</li>\n      <li>Inclua um arquivo <code>README.md</code> explicando o projeto, como executá-lo e as conclusões da sua análise.</li>\n      <li>Submeta o projeto em um repositório Git (GitHub, GitLab).</li>\n    </ul>\n  </li>\n</ol>\n\n<p>Este projeto é a sua oportunidade de demonstrar suas habilidades como um cientista de dados ou engenheiro de Machine Learning. Boa sorte!</p>",
        "examples": [],
        "summary": "O projeto final consiste em desenvolver um sistema de previsão de preços de imóveis, aplicando todo o fluxo de trabalho de Machine Learning: análise exploratória, pré-processamento, treinamento de múltiplos modelos, avaliação e comparação de resultados.",
        "estimated_time_minutes": 0
    }
]